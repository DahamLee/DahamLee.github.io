# 기초 통계 개념

* `모수`(Parameter - 모집단의 통계량을 모수라고 한다. 대부분 알려지지 않음)

* `통계량`(Statistics - 집단에서 추출한/관측된 표본에 담겨있는 통계적 특성치)

	ex) 표본평균, 표본분산, 표본 비율

값(value): 측정시점마다 변화
변수(variable): 측정 가능한 속성, 질적 속성, 양적 수량
관측점(Observation): 동시점 동객체에 대해 측정된 정보 집합

확률변수(Random Variable): 확률 변수에 대응하는 확률분포는 표본분포라 함

```
X - Y
독립변수 - 종속변수
설명변수 - 반응변수
예측변수 - 결과변수
위험인자 - 표적변수
```

**Y 변수가 있냐 없냐로 지도학습 비지도 학습으로 나뉜다.**

히스토그램: 가로축(계급구간), 세로축(도수)

* QQ(Quantile-Quantile) Plot
	* 샘플 Quantile(y축), 이론적 Quantile(x축)
	* 분석하고자하는

* 탐색적 데이터 분석(기술 통계)

* 자료(Data)
	* 범주형(Categorical)
		* 명목형(Norminal) - 성별, 혈액형
		* 순서형(Ordinal) - 비만도, 학점, 선호도
	* 수치형(Numerical)
		* 이산형(Discrete) - 멤버의 수
		* 연속형(Continuous) - 신장, 체중

* 범주형(Categorical data): 정성적, 질적 자료, **빈도 중심 Numerical 분석**
	* 명목형(Norminal data): 숫자로 바꾸어도 그 값이 크고 작음을 나타내는 것이 아니라 단순히 범주를 표시 - **카이제곱검정**
	* 순서형(Ordinal data): 범주의 순서가 상대적으로 비교가능, 대부분 수치형 자료를 그룹화하여 순서형 자료로 바꿈

* 수치형(Numerical data): 정략적, 양적 자료, 범위형, 비율형, 평균/분산 Numerical 분석
	* 이산형(Discrete data): 셀 수 있는 형태의 자료(Countable data)
	* 연속형(Continuous data): 연속적인 속성을 가지는 자료, 연속자료는 이산화를 통해 자연수 형태로 표시되는 경우가 많음


* 기술 통계(Descriptive Analysis)
	* 탐색적 자료 분석 (Exploratory Data Analysis)
	* 수집된 자료의 특성을 쉽게 파악할 수 있도록 자료를 정리, 분석하여 해석하기 쉬운 형태로 만드는 것
	* 숫자(통계량)로 표현
	* 도표나 차트로 표현
* 추론통계(Predictive Analysis)
	* 추정: 점추정, 구간 추정
	* 검정: 가설 검정

* 95% 신뢰구간(중심극한정리가 근간이 됨)
	* 모수(참값) 95번이 구간안에 들어가고 5번은 안들어간다.
	* 100(1-alpha)% 신뢰구간과 유의수준 alpha%과 일대일 관계

* 귀무가설: 관습적이고 보수적인 주장, 차이가 없다, 관계가 없다 등
* 대립가설: 적극적으로 입증하려는 주장, 차이가 있음, 관계가 있다 등 통계적 근거를 통해 입증하려는 주장
귀무가설이 기본 입장이며, 귀무가설이라고 하기에 너무 특이할 경우 채택

* 무죄추정의 원칙에서 출발, 혐의의 증거가 충분히 확보되면 유죄로 판결 내리는 것과 비슷

* 귀무가설과 대립가설의 차이를 검정통계량이라 하고 

* 검정통계량: 귀무가설이 참이라는 가정하에 계산하며, 검정통계량은 통계적 분포를 따른다

P-Value: 귀무가설이 참일 때, 관측자에 의한 검정통계량보다 극한값을 얻은 확률.
특이함의 정도 표현하며, 데이터가 특이할수록(대립가설에 가까울수록) P-Value가 작아짐

P-Value의 유의수준: 0.05 -> 1종 오류가 2종 오류보다 더 위험하기 때문에 1종 오류의 최대 허용치를 유의 수준이라고 한다.


### 상관 분석(Correlation Analysis)

* Add function - Hive Query: 

```case when temperature <= 15 then '01' else '02' end```

### 통계분석 단계
	1. 귀무가설 수립
	2. 검정통계량 계산(어떤 검정을 실시할 것인가를 결정)
	3. 귀무가설을 기각? 채택? (p값으로 결정)

### 관측척도(Scale of measurement)
	1. 연속변수(Continuous variable)
	2. 번주형변수(Categorical variable)

### Outcome
	* 독립인 두 집단의 평균 비교: 

### Machine Learning Algorithm
* Predicting the Future
* Modeling
	* Classification
		* Frequency Table
			* ZeroR
			* OneR
			* Naive Bayesian
			* Decision Tree
		* Covariance Matrix
			* Linear Discriminant Analysis
			* Logistic Regression
		* Similarity Function
			* K Nearest Neighbours
		* Others
			* Artificial Neural Network
			* Support Vector Machine
	* Regression
		* Frequency Table
			* Decision Tree
		* Covariance Matrix
			* Multiple Linear Regression
		* Similarity Functions
			* K Nearest Neighbours
		* Others
			* Artificial Neural Network
			* Support Vector Machine
	* Clustering
		* Hierarchical
			* Agglomerative (덩어리가 되는)
			* Divisive
		* Partitive
			* K Means
			* Self Organizing Map
	* Association Rules

### Linear Regression

Decription

* The Best fit line through all data points. 
* Predictions are numerical.

Advantages

* Easy to understand, you clearly see what the biggest drivers of the modest are.

Disadvantages

* Sometimes too simple to capture complex relationship between variables.
* Tendancy for the model to "overfit"

### Logistic Regression

Description

* The adaptation of linear regression to problem of classification (yes/no questions)

Advantages

* Also easy to understand.

Disadvantages

* Sometimes too simple to capture complex relationships between vairables
* Tendancy for the model to overfit

### Decision Tree

Description

* A graph that uses a branching method to match all possible outcomes of a decision

Advantages

* Easy to understand and implement.

Disadvantages

* Not often used on its own for prediction because it;s also often too simple and not powerful enough for complex data

### Random Forest

Description

* Takes the average of many decision trees, each of which is made with a sample of the data. Each tree is weaker than a full decision tree, but by combining them we get better overall performance.

Advantages

* A sort of "wisdom of the crowd". Tends to result in very high quality models. Fast to train.

Disadvantages

* Can be slow to output predictions relative to other algorithms.
* Not easy to understand predictions

### Gradient Boosting

Description

* Uses even weaker decision trees, that are increasingly focused on "hard" examples.

Advantages

* High performing

Disadvantages

* A small change in the feature set or training set can create radical changes in the model
* Not easy to understand predictions.

### Neural networks

Description

* Mimics the behavior of the brain. Neural networks are interconnected neurons that pass messages to each other. Deep learning uses several Layers of neural networks put one the after.

Advantages

* Can handle extremely complex tasks - no other algorithm comes close in image recognition.

Disadvantages

* Very, very slow to train, because they have so many layers. Require a lot of power.
* Almost impossible to understand prediction.
