## 기계학습의 종류

## Machine Learning 

* Unsupervised Learning
	* Clustering
* Supervised Learning
	* Classification
	* Regression
		
* 지도학습 (Supervised Learning)
	* 입력 및 출력 변수의 값을 이용하여 주어진 입력변수에 대한 출력변수의 값을 예측하는 모형을 개발
	* 회귀 및 분류 모형(regression and classification)
		* 회귀: 연속형 출력 변수
		* 분류: 범주형 출력 변수
	* 기법: 판별분석, 회귀분석, 로지스틱 회귀분석, 의사결정나무, 신경망

* 비지도학습 (Unsupervised Learning)
	* 출력변수가 없음
	* 입력 변수간의(혹은 내의) 관계를 탐색적으로 분석하여 의미 있는 정보 추출
	* 기법: 군집 분석, 연관성 분석, 주성분 / 인자분석

* Semi-supervised Learning
	* Semi-supervised learning is a class of supervised learning that make use of unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data

## Deep Learning
	* GAN(Generative Adversarial Networks)
		* Zero-sum game framework: Generator vs Discriminator
* Deep Learning의 문제점: 기본적으로 Supervised Learning 이므로 엄청난 양의 Labeled Data 필요

## Machine Learning 의 개요

### 모형의 평가
* 목적: 하나의 자료 분석 시 여러가지 가능한 모형을 적합시키게 되는데, 이중 최적의 모형을 선택하기 위함
* 모형의 평가 방법
	* 예측력: 얼마나 잘 예측하는가?
	* 해석력: 모형이 입력 / 출력 변수간의 관계를 잘 설명하는가?
	* 효율성: 얼마나 적은 수의 입력변수로 모형을 구축했는가?
	* 안정성: 모집단의 다른 자료에 적용했을 때 같은 결과를 주는가?

* 모형의 평가:
	* 어떤 모형이 랜덤하게 예측하는 모형보다 예측력이 우수한지, 그리고 고려된 모형들 중 어느 모형이 가장 좋은 예측력을 보유하고 있는지를 비교 / 분석

### 학습오차와 예측 오차
* 학습오차: 학습 자료로부터 구한 오차
* 예측오차: 미래의 자료로부터 구한 오차

* 지도 학습은 일반화에 관심을 둔다.
* 따라서, 학습오차보다는 예측오차에 더 많은 관심
* 즉, 지도 학습의 목적은 **예측오차를 최소화** 하는 모형의 구축에 있다.

### 과적합(Overfitting)
* 매우 복잡한 모형을 사용하여 학습오차를 매우 작게 한 경우 예측오차가 매우 커질 수 있는데, 이러한 현상을 과적합이라 함
* 예측오차는 미래의 자료에 대한 오차로서 실제로는 구할 수 없음

### Regularization 방법
* 학습자료를 이용하여 다양한 복잡도를 갖는 모형을 구축
* 각 모형의 학습오차에 **모형의 복잡도**에 대한 적절한 **penalty**를 더해 비용함수 생성
* 비용함수를 최소화하는 모형 선택

### 데이터 분할(Data Partioning) 방법
* 전체 자료를 훈련자료(training data)와 테스트자료(test data)로 나누고
* 훈련자료를 이용하여 모형 적합하고
* 적합된 모형을 테스트 자료에 적용하여 예측하며
* 테스트 자료에 대하여 평가 기준을 이용하여 평가

* 보통 특정한 분할에 의한 bias를 줄이기 위해 데이터에 대한 랜덤한 분할을 여러 번 반복(repetition) 실시 (랜덤포레스트 처럼)
* 방법에 따라 훈련, 검증 (validation), 테스트 자료로 나누어서 검증 데이터를 이용하여 모형을 최적화 하는 경우도 있음

### 교차 확인 (Cross Validation)
데이터 분할 방법의 일반화 개념

* K-fold Cross Validation
	* 데이터를 k개의 set으로 분할
	* 각 j에 대해 j번째 set을 제외한 나머지를 이용하여 모형 적합
	* j 번째 set을 테스트 자료로 이용하여 예측 오차를 구함
	* k 개의 예측 오차의 평균을 예측 오차로 활용하여 최적 모형 선택

### Regression Model의 평가
* MSE (Mean Squared Error)
* RMSE (Root Mean Squared Error)
* MAE (Mean Absolute Error)
* MAPE (Mean Absoulte Percent Error)


### Classification Model의 평가1
* Confusion Matrix
	* 레이블 0, 1을 가진 데이터를 분류한다고 할 때 관심 범주를 1이라고 한다.
* True Positives(TP): 1인 레이블을 1이라 하는 경우를 True Positive라고 한다. -> 관심 범주를 정확하게 분류한 값
* False Negatives(FN): 1인 레이블을 0이라 하는 경우를 False Negative라고 한다. -> 관심 범주가 아닌 것으로 잘못 분류함.
* False Positives(FP): 0인 레이블을 1이라 하는 경우를 False Positive라고 한다. -> 관심 범주라고 잘못 분류함.
* True Negatives(TN): 0인 레이블을 0이라 하는 경우를 True Negative라고 한다. -> 관심 범주가 아닌 것을 정확하게 분류

### Classification Model의 평가2
*  Accuracy (정확도) = (TP + TN) / (TP+TN+FP+FN)
*  Precision (정밀도) = TP / (TP + FP)
*  Recall (재현율) = TP / (TP + FN)
*  F-measure = (1+Beta^2) * Precision * Recall / (Beta^2 * Precision + Recall)
*  eg. Beta가 1이면 2 * Precision * Recall / (Precision + Recall)

### ROC(Receiver Operating Characteristic) Curve
* 모형의 성능을 Sensitivity(민감도, TPR)와 Specificity(특이도)에 의해 판단하고자 하는 곡선
* X축에는 FPR(False Positive Rate), Y축에는 TPR(True Positive Rate) 지정하고 그린 곡선
* AUC(Area Under Curve)가 클수록 (1에 가까울 수록) 좋은 모델

### 대표 Classification 알고리즘 종류
* K-nearst neighbors: 
	* K개의 가장 거리가 가까운 데이터의 클래스로 대표 선정
	* 거리를 기반으로 가중치 제공 및 변수 추출로 최적화
* Naive Bayes:
	* 베이즈 정리와 조건부 확률을 이용하여 발생 확률 예측
	* 변수간 독립을 가정하며, 최대 확률을 갖는 클래스 선정
* Logistic Regression:
	* 이진 클래스의 발생 확률을 Logit 함수에 맞춰 예측(0~1)
	* 확률은 변수의 선형 조합으로 모델링 됨
* Support Vector Machine
	* 클래스를 가장 잘 분리하는 경계(Hyper Plane)로 분리
	* 경계의 최대 마진을 찾으며, 커널 트릭으로 최적화
* Decision Tree
	* 계층 구조의 결정 트리 형태로 분류기를 생성
	* 노드에서 가장 정보획득량이 많은 변수와 값으로 최적화
* Random Forest
	* 다수의 학습된 트리에서 평균 예측으로 분류
	* 앙상블 방법인 베깅을 이용하여 일반화 성능 향상
* Gradient Boosting Tree
	* 약한 트리를 개선해 가면서 트리 모형을 최적화
	* 앙상블 방법인 부스팅을 이용하여 틀린 클래스에 가중치

### KNN(K Nearest Neighbor) - supervised learning
* Test 데이터와 가까운 k개의 Train 데이터의 y값들을 비교하여 가장 많은 class로 예측하는 방법
* Nx를 x와 가까운 k개의 Train 데이터들의 집합이라고 할 때 (이를 x의 k근방이라고 부름) 예측값 Y는,
y_hat(x) = arg max [Sigma I (yj = l)] l -> 1~L
* K 최근접 이웃 알고리즘은 분류나 회귀에 사용되는 비모수 방식
* 두 경우 모두 입력이 특징, 공간 내 k개의 가장 가까운 훈련 데이터로 구성
* 출력은 KNN이 분류로 사용되었는지 또는 회귀로 사용되었는지에 따라 다르다.
* KNN **분류**에서 출력은 소속된 항목이다. 객체는 K개의 최근접 이웃 사이에서 가장 공통적인 항목에 할당되는 객체로 과반수 의결에 의해 분류된다.
* KNN **회귀**에서 출력은 객체의 특성 값이다. 이 값은 K개의 최근접 이웃이 가진값의 평균이다.

### Naive Bayes Classification - supervised learning
* Bayes 정리에 기반을 두는 분류기로 가장 확률이 높은 곳으로 단순 분류함
* Naive Bayes Classification 개념
	* n 개 특성을 가지는 데이터 벡터: x = (x1, ..., xn)
	* K 개의 가능한 확률적 결과들 (클래스)의 확률:
	* 각 Xi 값들을 독립으로 가정하면


### Logistic Regression - supervised learning
* 분석하고자 하는 대상들이 두 집단 혹은 그 이상의 집단으로 나누어진 경우에 개별 관측치들이 어느 집단에 분류될 수 있는가를 분석하고 이를 예측하는 모형을 개발하는데 사용되는 기법

* **Logistic Response Function**
* P = 1 / (1+exp(-beta0 - beta1*x1+...))

* **Odds** (승산: 성공확률 / 실패확률)
* odds = p / (1 - p)
* 사건의 odds가 주어졌을 때 사건의 확률을 계산할 수 있음
* odds = p / (1 - p) = exp(beta0 + beta1*x1+...)
* **Logit**: log(odds) = beta0 + beta1 * x1 + beta2 * x2...

* 로지스틱 회귀분석이란 로짓을 종속변수로 정의하고, 이 로짓과 q개의 예측변수와의 관계를 선형으로 모형화한 것을 말함

* p: 성공확률 (0~1)
	* X=x 인 경우 Y=success (Y=1) 확률 p = Pr(Y=1|X=x)

* Odds: 성공확률 / 실패확률
	* odds = p / (1-p)
* Logit: Odds에 자연로그를 취한 것, log(Odds)

### 로지스틱 회귀분석 모델링
* 종속변수로 Y를 사용하는 대신에 로짓함수를 사용
* 집단 1에 속하는 확률인 p를 구한다
* p는 [0,1] 사이의 값을 갖는다.

### Support Vector Machine
* 기계 학습의 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도 학습 모델로 주로 분류와 회귀 분석을 위해 사용
* 만들어진 분류 모델은 데이터가 사상된 공간에서 경계로 표현
* SVM 알고리즘은 그 중 가장 큰 폭을 가진 경계를 찾는 알고리즘

* SVM의 활용
* SVM은 선형 분류와 더불어 비선형 분류에서도 사용
* 선형 분류: 내적(inner product) 사용
* 비선형 분류: Kernal

* SVM 처리 과정
* Support Vector를 기준으로 margin을 최대화 하는(가장 잘 분류하는) hyper plane 찾기

* train data를 이용하여 f(x) = <w,x> + b 로 정의되는 선형 판별함수를 얻은 후, 그 부호를 이용하여 새로운 입력 x에 대한 출력 값을 예측하는 것
* w의 해를 얻기 위하여 SVM은 다음과 같은 이차함수를 최적화
* min (1/2) <w,w> + C Sigma ksi
	 
### Decision Tree
* 주어진 입력값에 대해 출력값을 예측하는 모형으로 결정트리를 사용
	* 이항, 다항 Classification 뿐만 아니라 Regression에서도 사용 가능
* If then else 스타일의 룰 형태로 계층적 트리를 쌓아감
* 트리의 각 노드에서 클래스를 가장 잘 분리하는 변수와 값을 찾으며 학습
	* 불순도를 감소시키는 방향으로 분리(impuruty 감소)

### Decision Tree의 장단점
* 장점1. 나무 구조로 시각화하여 결과를 해석하고 이해하기 쉬움
* 장점2. 자료를 가공할 필요가 거의 없음
	* 비모수적 모형(가정 필요 없음)
	* 수치형 및 범주형 데이터 모두 적용 가능
	* 스케일에 민감하지 않음
	* 이상치(Outlier)에 대해 상대적으로 덜 민감

* 단점1. 과적함(Overfitting)의 위험이 있음
* 단점2. 최적의 결정 트리를 보장하지 못함
	* 노드 별 부분 최적값(local optimum)을 찾으며, 결과가 불안정적일 수 있음.
	* 해결: 가지치기(Pruning) 및 앙상블 모델(Random Forest)등

### Decision Tree 용어
* 뿌리마디(root node): 시작되는 마디로 전체 자료를 포함
* 자식마디(child node): 하나의 마디로부터 분리되어나간 2개 이상의 마디들
* 부모마디(Parent node): 주어진 마디의 상위마디
* 끝마디(terminal node): 자식마디가 없는 마디
* 중간마디(internal node): 부모마디와 자식마디가 모두 있는 마디

* 가지(branch): 뿌리마디로부터 끝마디까지 연결된 마디들(화살표)

* 깊이(depth): 뿌리마디부터 끝마디까지의 중간마디들의 수

### Decision Tree의 형성 과정
* 최적의 방향으로 나무를 성장시키고
* 적정한 정지규칙에서 성장을 중단시킴
* 불필요한 가지는 가지치키(pruning)을 통해 제거

* 최적 형성기준은 Impurity measure(불순도)를 최소화 하는 방향으로 분리 기중(split)을 정함
* 각 변수 별로 값을 바꾸어 가면서 불순도를 비교(Brute force exploration)

### 과적합 (Overfitting)
* 학습데이터에 대해서는 좋은 결과를 내나, 학습에 사용하지 않은 테스트 데이터에 대해서는 정확한 예측을 못하는 것
* 학습데이터가 부족하거나, 학습데이터에 잡음이 있는 경우, 학습데이터에 너무 특화될 때 발생됨

### Decision Tree Stopping Rule(정지규칙)
* 규칙 조건에 도달한 마디가 끝 마디가 되도록 성장을 미리 억제
* 일반적인 Stopping Rule: 
	* 노드의 모든 데이터가 하나의 클래스만 남은(순도 100% 인 경우)
	* 트리의 깊이(depth)가 지정한 값에 도달함
	* 노드의 크기가 최소 노드 보다 작음(해당 노드에 포함된 데이터의 수가 작음)
	* 불순도의 감소량이 지정된 값보다 적음(가지를 더 만드는 게 불순도 감소에 적게 기여)